"""
    Shows both an initial average gradient, as well as the final learned schedule.
"""

import sys
import pickle
import numpy as np
from collections import defaultdict

from rsub import *
import matplotlib.pyplot as plt

sys.path.append('/home/bjohnson/software/autograd/')
from funkyyak import grad, kylist, getval

sys.path.append('/home/bjohnson/software/hypergrad')
from hypergrad.data import load_data_dicts
from hypergrad.nn_utils import VectorParser
from hypergrad.optimizers import adam, sgd_parsed
from hypergrad.util import RandomState

def logit(x): 
    return 1 / (1 + np.exp(-x))

def inv_logit(x): 
    return -np.log(1 / x - 1)

def logsumexp(X, axis):
    max_X = np.max(X)
    return max_X + np.log(np.sum(np.exp(X - max_X), axis=axis, keepdims=True))

# --
# Fixed params

layer_sizes = [784, 50, 50, 50, 10]
batch_size  = 200
N_iters     = 100
N_classes   = 10
N_train     = 10000
N_valid     = 10000
N_tests     = 10000
thin        = 1 # np.ceil(N_iters/N_learning_checkpoint) # Detailed learning curves.
N_learning_checkpoint = 20

# --
# Initial values of learned hyper-parameters

init_log_L2_reg      = -100.0
init_log_alphas      = -1.0
init_invlogit_betas  = inv_logit(0.5)
init_log_param_scale = -3.0

# --
# Superparameters

seed = 0

# --
# Helpers

def fill_parser(parser, items):
    partial_vects = [np.full(parser[name].size, items[i]) for i, name in enumerate(parser.names)]
    return np.concatenate(partial_vects, axis=0)

# --
# Make NN functions

parser = VectorParser()
for i, shape in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):
    parser.add_shape(('weights', i), shape)
    parser.add_shape(('biases', i), (1, shape[1]))

def pred_fun(W_vect, X):
    """Outputs normalized log-probabilities."""
    W = parser.new_vect(W_vect)
    cur_units = X
    N_iter = len(layer_sizes) - 1
    for i in range(N_iter):
        cur_W = W[('weights', i)]
        cur_B = W[('biases',  i)]
        cur_units = np.dot(cur_units, cur_W) + cur_B
        if i == (N_iter - 1):
            cur_units = cur_units - logsumexp(cur_units, axis=1)
        else:
            cur_units = np.tanh(cur_units)
    
    return cur_units

def loss_fun(W_vect, X, T, L2_reg=0.0):
    # TODO: consider treating L2_reg as a matrix
    log_prior = -np.dot(W_vect * L2_reg, W_vect)
    log_lik = np.sum(pred_fun(W_vect, X) * T) / X.shape[0]
    return - log_prior - log_lik

def frac_err(W_vect, X, T):
    preds = np.argmax(pred_fun(W_vect, X), axis=1)
    return np.mean(np.argmax(T, axis=1) != preds)


# --
# Run

train_data, valid_data, test_data = load_data_dicts(N_train, N_valid, N_tests)

N_weight_types = len(parser.names)

hyperparams = VectorParser()
hyperparams['log_param_scale'] = np.full(N_weight_types, init_log_param_scale)
hyperparams['log_alphas']      = np.full((N_iters, N_weight_types), init_log_alphas)
hyperparams['invlogit_betas']  = np.full((N_iters, N_weight_types), init_invlogit_betas)

fixed_hyperparams = VectorParser()
fixed_hyperparams['log_L2_reg'] = np.full(N_weight_types, init_log_L2_reg)

cur_primal_results = {}

def primal_optimizer(hyperparam_vect, meta_epoch):
    
    def indexed_loss_fun(w, L2_vect, i_iter):
        rs = RandomState((seed, meta_epoch, i_iter))  # Deterministic seed needed for backwards pass.
        idxs = rs.randint(N_train, size=batch_size)
        return loss_fun(w, train_data['X'][idxs], train_data['T'][idxs], L2_vect)
    
    # learning_curve_dict = defaultdict(list)
    
    def callback(x, v, g, i_iter):
        pass
        # if i_iter % thin == 0 or i_iter == (N_iters - 1) or i_iter == 0:
            # learning_curve_dict['learning_curve'].append(loss_fun(x, **train_data))
            # learning_curve_dict['grad_norm'].append(np.linalg.norm(g))
            # learning_curve_dict['weight_norm'].append(np.linalg.norm(x))
            # learning_curve_dict['velocity_norm'].append(np.linalg.norm(v))
            # learning_curve_dict['iteration'].append(i_iter + 1)
    
    cur_hyperparams = hyperparams.new_vect(hyperparam_vect)
    
    rs = RandomState((seed, meta_epoch))
    
    W0 = fill_parser(parser, np.exp(cur_hyperparams['log_param_scale']))
    W0 *= rs.randn(W0.size)
    
    alphas = np.exp(cur_hyperparams['log_alphas'])
    betas  = logit(cur_hyperparams['invlogit_betas'])
    L2_reg = fill_parser(parser, np.exp(fixed_hyperparams['log_L2_reg']))
    W_opt  = sgd_parsed(grad(indexed_loss_fun), kylist(W0, alphas, betas, L2_reg), parser, callback=callback)
    cur_primal_results['weights'] = getval(W_opt).copy()
    # cur_primal_results['learning_curve'] = getval(learning_curve_dict)
    return W_opt


def hyperloss(hyperparam_vect, meta_epoch):
    W_opt = primal_optimizer(hyperparam_vect, meta_epoch)
    return loss_fun(W_opt, **train_data)


def meta_callback(hyperparam_vect, meta_epoch, metagrad=None):
    
    # !! Probably don't need
    cur_hyperparams = hyperparams.new_vect(hyperparam_vect.copy())
    for field in cur_hyperparams.names:
        meta_results[field].append(cur_hyperparams[field])
    
    # Compute metrics
    train_loss = loss_fun(cur_primal_results['weights'], **train_data)
    valid_loss = loss_fun(cur_primal_results['weights'], **valid_data)
    test_loss  = loss_fun(cur_primal_results['weights'], **test_data)
    test_err   = frac_err(cur_primal_results['weights'], **test_data)
    
    # Logging
    meta_results['train_loss'].append(train_loss)
    meta_results['valid_loss'].append(valid_loss)
    meta_results['test_loss'].append(test_loss)
    meta_results['test_err'].append(test_err)
    # meta_results['learning_curves'].append(cur_primal_results['learning_curve'])
    # meta_results['example_weights'] = cur_primal_results['weights']
    
    print "Meta Epoch {0} Train loss {1:2.4f} Valid Loss {2:2.4f} Test Loss {3:2.4f} Test Err {4:2.4f}".format(
        meta_epoch, 
        train_loss,
        valid_loss,
        test_loss,
        test_err,
    )


meta_alpha  = 0.04 # Meta-learning rate
meta_epochs = 50 # Number of epochs

meta_results = defaultdict(list)

hyperloss_grad = grad(hyperloss)
final_result = adam(hyperloss_grad, hyperparams.vect, meta_callback, meta_epochs, meta_alpha)

meta_callback(final_result, meta_epochs)
parser.vect = None # No need to pickle zeros

pickle.dump((meta_results, parser), open('results-v2.pkl', 'w'))

# --
# Plot

results, parser = pickle.load(open('./results-v2.pkl'))

for cur_results, name in zip(results['log_alphas'][-1].T, parser.names):
    if name[0] == 'weights':
        _ = plt.plot(np.exp(cur_results), 'o-', label=name[1] + 1)

_ = plt.legend(loc='upper right')
show_plot()
